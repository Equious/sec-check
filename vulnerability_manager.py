# vulnerability_manager.py
import logging
import json
from typing import List, Dict, Any, Generator
from config import VULNERABILITY_CHECKLIST_FILE
from utils import load_json_file
from llm_handler import LLMHandler

logger = logging.getLogger(__name__)

class VulnerabilityManager:
    def __init__(self, llm_handler: LLMHandler):
        self.llm_handler = llm_handler
        self.full_checklist: List[Dict[str, Any]] = self._load_and_flatten_checklist()

    def _flatten_checklist_items(self, items: List[Dict[str, Any]], current_categories: List[str], current_descriptions: List[str]) -> Generator[Dict[str, Any], None, None]:
        """
        Recursively flattens the nested checklist structure.
        Yields individual vulnerability items with inherited context.
        """
        for item in items:
            # These are the actual checkable vulnerabilities with an 'id'
            if "id" in item and "question" in item:
                # Create a new dict for the flattened item to avoid modifying original
                flattened_item = item.copy() # Start with all keys from the item
                flattened_item["name"] = item.get("question", "N/A") # Use question as name
                # 'description' is already specific to the item
                flattened_item["inherited_categories"] = list(current_categories)
                flattened_item["inherited_descriptions"] = list(current_descriptions)
                
                # Map existing fields and note missing ones that would be beneficial
                flattened_item["keywords"] = item.get("tags", []) # Use 'tags' as 'keywords'
                
                # Fields that would be very beneficial to add to your checklist:
                # flattened_item["severity"] = item.get("severity", "Medium") # Placeholder
                # flattened_item["cwe"] = item.get("cwe", None)
                # flattened_item["swc_id"] = item.get("swc_id", None)
                # flattened_item["applicability_conditions"] = item.get("applicability_conditions", {})
                # flattened_item["example_vulnerable_code"] = item.get("example_vulnerable_code", "")
                # flattened_item["example_secure_code"] = item.get("example_secure_code", "")
                # flattened_item["poc_exploit_logic_template"] = item.get("poc_exploit_logic_template", [])

                yield flattened_item
            
            # If the item itself is a category containing more data
            elif "category" in item and "data" in item and isinstance(item["data"], list):
                new_categories = current_categories + [item["category"]]
                new_descriptions = current_descriptions + [item.get("description", "")]
                yield from self._flatten_checklist_items(item["data"], new_categories, new_descriptions)


    def _load_and_flatten_checklist(self) -> List[Dict[str, Any]]:
        nested_checklist_data = load_json_file(VULNERABILITY_CHECKLIST_FILE)
        if nested_checklist_data is None:
            logger.error("Vulnerability checklist could not be loaded. Exiting.")
            raise FileNotFoundError(f"Could not load {VULNERABILITY_CHECKLIST_FILE}")
        if not isinstance(nested_checklist_data, list):
            logger.error(f"Vulnerability checklist at {VULNERABILITY_CHECKLIST_FILE} is not a list. Exiting.")
            raise TypeError(f"Checklist at {VULNERABILITY_CHECKLIST_FILE} must be a list.")

        flattened_list = []
        try:
            flattened_list = list(self._flatten_checklist_items(nested_checklist_data, [], []))
            print(f"Successfully loaded and flattened {len(flattened_list)} vulnerability items from checklist.")
        except Exception as e:
            logger.error(f"Error during checklist flattening: {e}")
            raise
        
        return flattened_list

    def get_applicable_vulnerabilities(
        self,
        project_summary: str,
        contract_contents: Dict[str, str], # filename: content
        detected_standards: List[str] # e.g., ["ERC20", "ERC721", "Upgradeable"]
    ) -> List[Dict[str, Any]]:
        """
        Step 2: Determine which checklist items are appropriate using LLM.
        Relies more on LLM due to lack of explicit applicability conditions in checklist.
        """
        print("Determining applicable vulnerabilities for the project using LLM...")

        if not self.full_checklist:
            logger.warning("Full checklist is empty. Cannot determine applicable vulnerabilities.")
            return []

        # Construct a prompt for the LLM
        # We provide project summary, contract snippets (maybe just key ones or summaries),
        # and the list of all flattened vulnerabilities.

        vulnerability_info_for_llm = []
        for v_item in self.full_checklist:
            # Construct a comprehensive name/context for the LLM
            category_path = " -> ".join(v_item.get("inherited_categories", []))
            full_name_for_llm = f"{category_path} - {v_item.get('name', 'Unnamed Vulnerability')}"
            vulnerability_info_for_llm.append(
                f"- ID: {v_item['id']}\n  Context & Question: {full_name_for_llm}\n  Description: {v_item.get('description', 'N/A')}\n  Tags: {', '.join(v_item.get('keywords', []))}"
            )
        
        # Limiting contract content for the prompt
        limited_contracts_for_prompt = {k: v[:2000] + ("..." if len(v) > 2000 else "") for i, (k, v) in enumerate(contract_contents.items()) if i < 3}
        contract_context_for_llm = "\n\nKey Contract Snippets:\n"
        for filename, content in limited_contracts_for_prompt.items():
            contract_context_for_llm += f"--- {filename} ---\n{content}\n\n" ## QQ

        prompt = f"""
        You are an AI Smart Contract Security Auditor.
        Given the following project information and a list of potential smart contract vulnerabilities (presented as questions/checks),
        determine which vulnerabilities are most likely applicable and relevant to investigate for this specific project.
        Consider the project's likely purpose, detected standards (if any), and the nature of the code snippets.
        For example, if the project doesn't seem to involve NFTs, NFT-specific vulnerabilities are not applicable.

        Project Summary:
        {project_summary}

        Detected Standards/Patterns in Project:
        {', '.join(detected_standards) if detected_standards else 'None explicitly detected yet.'}
        
        {contract_context_for_llm}

        Potential Vulnerabilities List (ID, Context/Question, Description, Tags):
        {chr(10).join(vulnerability_info_for_llm)}

        Format your response as a JSON object with a single key "applicable_vulnerabilities",
        which is a list of objects, where each object has "id" (from the provided list) and "reasoning"
        (explaining why it's applicable to THIS project).
        Focus on relevance. If a vulnerability is very generic, only include it if the project context strongly suggests it.
        Example:
        {{
          "applicable_vulnerabilities": [
            {{ "id": "SOL-AM-DOSA-1", "reasoning": "The project involves token withdrawals, so the withdrawal pattern is critical to prevent DOS." }},
            {{ "id": "SOME_OTHER_ID", "reasoning": "Explanation here, connecting the vulnerability to specific aspects of the provided project information." }}
          ]
        }}
        """
        logger.debug(f"Prompt for determining applicable vulnerabilities (length: {len(prompt)}):\n{prompt[:1000]}...")

        response_text = self.llm_handler.generate_text(prompt, temperature=0.2, max_output_tokens=20000, hint=True)
        logger.debug(f"LLM response for applicable vulnerabilities: {response_text}")

        applicable_vulnerabilities = []
        try:
            parsed_response = json.loads(response_text)
            llm_selected_items = parsed_response.get("applicable_vulnerabilities", [])
            
            llm_selected_ids_map = {item['id']: item.get('reasoning', 'No specific reasoning provided by LLM.') 
                                   for item in llm_selected_items if 'id' in item}
            
            for vuln_item in self.full_checklist:
                if vuln_item["id"] in llm_selected_ids_map:
                    # Add reasoning from LLM to the vulnerability item
                    vuln_item['llm_applicability_reasoning'] = llm_selected_ids_map[vuln_item["id"]]
                    applicable_vulnerabilities.append(vuln_item)
            
            print(f"LLM identified {len(applicable_vulnerabilities)} potentially applicable vulnerabilities.")

        except json.JSONDecodeError:
            logger.error(f"Failed to parse LLM response as JSON for applicable vulnerabilities. Response: {response_text}")
            # Fallback: Could return all, or none, or try a simpler heuristic.
            # For now, let's be conservative and return none if LLM fails to give structured output.
            return []
        except Exception as e:
            logger.error(f"An error occurred processing LLM response for applicable vulnerabilities: {e}")
            return []

        return applicable_vulnerabilities

# Example usage (optional, for testing this file directly)
if __name__ == "__main__":
    logging.basicConfig(level=logging.DEBUG)
    # Create a dummy vulnerability_checklist.json with the user's structure for testing
    dummy_checklist_content = """
    [
        {
            "category": "Attacker's Mindset",
            "description": "General check items for main attack types.",
            "data": [
                {
                    "category": "Denial-Of-Service(DOS) Attack",
                    "description": "Attackers overload a system...",
                    "data": [
                        {
                            "id": "SOL-AM-DOSA-1",
                            "question": "Is the withdrawal pattern followed to prevent denial of service?",
                            "description": "To prevent denial of service attacks during withdrawals...",
                            "remediation": "Implement withdrawal pattern best practices...",
                            "references": [],
                            "tags": ["withdrawal", "dos"]
                        },
                        {
                            "id": "SOL-AM-DOSA-2",
                            "question": "Is there a minimum transaction amount enforced?",
                            "description": "Enforcing a minimum transaction amount...",
                            "remediation": "Disallow transactions below a certain threshold...",
                            "references": [],
                            "tags": ["transaction", "dos", "spam"]
                        }
                    ]
                },
                {
                    "category": "NFT Specific",
                    "description": "Vulnerabilities related to NFTs.",
                    "data": [
                        {
                            "id": "NFT-SPEC-001",
                            "question": "Is enumerable NFT safe from gas issues?",
                            "description": "ERC721Enumerable can be costly.",
                            "remediation": "Avoid if not needed.",
                            "references": [],
                            "tags": ["nft", "erc721", "gas"]
                        }
                    ]
                }
            ]
        }
    ]
    """
    # Make sure VULNERABILITY_CHECKLIST_FILE points to a valid file for the test
    # For this example, let's write it temporarily
    test_checklist_path = "temp_checklist_test.json"
    with open(test_checklist_path, "w") as f:
        f.write(dummy_checklist_content)
    
    # Temporarily override config for this test
    original_checklist_file = VULNERABILITY_CHECKLIST_FILE
    import config
    config.VULNERABILITY_CHECKLIST_FILE = test_checklist_path

    try:
        # Dummy LLM Handler for testing without API calls
        class DummyLLMHandler:
            def generate_text(self, prompt: str, temperature: float = 0.3, max_output_tokens: int = 8192) -> str:
                print("DummyLLMHandler called. Returning predefined applicable vulnerabilities.")
                # Simulate LLM identifying SOL-AM-DOSA-1 and NFT-SPEC-001 as applicable
                return json.dumps({
                    "applicable_vulnerabilities": [
                        {"id": "SOL-AM-DOSA-1", "reasoning": "Project involves withdrawals."},
                        {"id": "NFT-SPEC-001", "reasoning": "Project summary mentioned NFTs."}
                    ]
                })

        llm_dummy = DummyLLMHandler()
        manager = VulnerabilityManager(llm_dummy)
        
        print(f"Total flattened checklist items: {len(manager.full_checklist)}")
        for item in manager.full_checklist:
            print(f"  ID: {item['id']}, Name: {item['name']}, Categories: {item.get('inherited_categories')}")

        # Test get_applicable_vulnerabilities
        dummy_project_summary = "This is a DeFi project with NFT staking and withdrawal features."
        dummy_contract_contents = {"contracts/MyToken.sol": "contract MyToken is ERC20 {}", "contracts/NFTStaking.sol": "contract NFTStaking uses ERC721 {}"}
        dummy_detected_standards = ["ERC20", "ERC721", "DeFi"] # Assume these were detected

        applicable = manager.get_applicable_vulnerabilities(
            dummy_project_summary,
            dummy_contract_contents,
            dummy_detected_standards
        )
        print(f"\nApplicable vulnerabilities identified ({len(applicable)}):")
        for app_vuln in applicable:
            print(f"  ID: {app_vuln['id']}, Name: {app_vuln['name']}, LLM Reasoning: {app_vuln.get('llm_applicability_reasoning')}")
        
        assert len(applicable) == 2
        assert applicable[0]['id'] == "SOL-AM-DOSA-1"
        assert applicable[1]['id'] == "NFT-SPEC-001"

    except Exception as e:
        logger.error(f"Error in VulnerabilityManager test: {e}", exc_info=True)
    finally:
        # Clean up
        import os
        os.remove(test_checklist_path)
        config.VULNERABILITY_CHECKLIST_FILE = original_checklist_file # Restore original path